\input{../../tex_header}

\title{Chapter 17 Solusion}
\date{12/28/2021}

\begin{document}
\maketitle

\section*{17.1}

\subsection*{17.1-1}

No.
Consider we operate $\proc{Multpush}(S,n)$ $n$ times.
Such $n$ operations cost $\Theta(n^2)$, 
so the amortized cost is $\Theta(n)$.

Actually, we can $\proc{Multpush}$ incredible large amount of items,
so $O(1)$ of course cannot be bound on the amortized cost
of stack operations.

\subsection*{17.1-2}

Consider a $k$-bit counter where each bit in the counter is $1$.
Now, we perform $\proc{Increment}$ which flips $k+1$ bits.
Then, we perform $\proc{Decrement}$ which flips $k+1$ bits again.
Hence perform a sequence of length $n$ operations 
$\langle \proc{Increment}, \proc{Decrement}, 
\proc{Increment}, \proc{Decrement}, \cdots \rangle$
cost $\Theta(nk)$ in total.

\subsection*{17.1-3}

\begin{equation*}
    n + \sum\limits_{i = 1}^{\lfloor \lg n \rfloor} (2^i - 1)
    \leq n + \sum\limits_{i = 0}^{\lg n} 2^i
    = n + 2^{\lg n + 1} - 1 = n + 2n - 1 = 3n - 1
\end{equation*}

Hence the amortized cost per operation is $O(1)$.

\section*{17.2}

\subsection*{17.2-1}

\begin{tabular}{c|c|c}
    operation & actual cost & amortized cost \\
    \hline
    \proc{Push} & 1     & 2 \\
    \proc{Pop}  & 1     & 2 \\
    Copy        & $s$   & 0 \\
\end{tabular}

where $s$ is the stack size when it is called 
which has an upper bound $k$.

Each operation ($\proc{Push}$ or $\proc{Pop}$) 
charges an amortized cost of $2$ and actual use $1$.
After $k$ operations, we have $k$ credits,
and copy operation cost at most $k$.
Hence we conclude the total amortized cost 
is greater than the total actual cost at all times.

\subsection*{17.2-2}

Let the amortized cost of each operation be $3$.
We want to show that
\begin{equation*}
    \sum\limits_{i = 1}^{n} \hat{c_i} \geq \sum\limits_{i = 1}^{n} c_i
\end{equation*}
for all integers $n$
where 
\begin{equation*}
    c_i = 
    \begin{cases}
        i & \text{if $i$ is an exact power of 2,} \\
        1 & \text{otherwise}
    \end{cases}
\end{equation*}
and $\hat{c_i} = 3$ for all integers $i$.
That is we want to show that
\begin{equation*}
    3n \geq n + \sum\limits_{i = 1}^{\lfloor \lg n \rfloor} (2^i - 1).
\end{equation*}
By exercise 17.1-3, we have 
\begin{equation*}
    n + \sum\limits_{i = 1}^{\lfloor \lg n \rfloor} (2^i - 1)
    \leq 3n - 1.
\end{equation*}
Hence the amortized cost per operation is $O(1)$.

\subsection*{17.2-3}

As the hint mentioned,
we keep a pointer to the high-order $1$
and maintain it during the operations.
In each $\proc{Increment}$ operation,
we check if we the high-order $1$ moved to a higher order.

Fliping a bit charges $1$.
Moving the pointer to the high-order $1$ charges $\$1$.
Let the amortized cost of each $\proc{Increment}$ operation be $\$4$,
and let the amortized cost of each $\proc{Reset}$ operation be $\$1$.
When we set a bit to $1$, we actually cost $\$1$ and retain $\$2$ as credits
for the purpose of setting to $0$ and resetting.
If we need to update pointer, we charge another $\$1$.
Hence amortized cost of each $\proc{Increment}$ operation is $\$4$.
Each $\proc{Reset}$ operation need to move the pointer to $-1$,
so it costs $\$1$.

\begin{minted}[xleftmargin=20pt,linenos]{cpp}
struct Counter
{
    int length;
    std::vector<bool> bits;
    int high_order_one;

    Counter(int length) : length(length), 
        bits(length, 0), high_order_one(-1) {}
};

void Increment(Counter& counter)
{
    int i;
    i = 0;
    while (i < counter.length && counter.bits[i] == 1)
    {
        counter.bits[i] = 0;
        ++i;
    }
    if (i < counter.length)
    {
        counter.bits[i] = 1;
        counter.high_order_one = std::max(i, counter.high_order_one);
    }
    else
    {
        // overflow
        counter.high_order_one = -1;
    }
}

void Reset(Counter& counter)
{
    int i;
    for (i = 0; i < counter.length; ++i)
    {
        counter.bits[i] = 0;
    }
    counter.high_order_one = -1;
}
\end{minted}

\section*{17.3}

\subsection*{17.3-1}

Let $\Phi'(D_i) = \Phi(D_i) - \Phi(D_0)$.
Clearly, $\Phi'(D_0) = 0$.
We claim the amortized costs using $\Phi'$ are the same
as the amortized costs using $\Phi$.

\begin{equation*}
    \begin{split}
        \hat{c_i} & = c_i + \Phi'(D_i) - \Phi(D_{i-1}) \\
        & = c_i + (\Phi(D_i) - \Phi(D_0)) - (\Phi(D_{i-1}) - \Phi(D_0)) \\
        & = c_i + \Phi(D_i) - \Phi(D_{i-1}) \\
    \end{split}
\end{equation*}
    
\subsection*{17.3-2}

Let $\Phi(D_0) = 0$ and $\Phi(D_i) = 2(i - 2^{\lfloor \lg i \rfloor})$
for $i \geq 1$.

\begin{equation*}
\begin{split}
    \Phi(D_i) - \Phi(D_{i - 1}) 
    & = 2(i - 2^{\lfloor \lg i \rfloor}) - 2((i - 1) - 2^{\lfloor \lg (i - 1) \rfloor}) \\
    & = 2 - 2(2^{\lfloor \lg i \rfloor} - 2^{\lfloor \lg (i - 1) \rfloor}) \\
\end{split}
\end{equation*}

Note that
\begin{equation*}
    c_i = 
    \begin{cases}
        i & \text{if $i$ is an exact power of 2,} \\
        1 & \text{otherwise} \\
    \end{cases}
\end{equation*}

\textbf{Case 1.}
$i$ is an exact power of 2.

\begin{equation*}
\begin{split}
    \Phi(D_i) - \Phi(D_{i - 1}) 
    & = 2 - 2(i - \frac{i}{2}) \\
    & = 2 - i \\
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
    \hat{c_i} & = c_i + \Phi(D_i) - \Phi(D_{i - 1}) \\
    & = i + 2 - i \\
    & = 2 \\
\end{split}
\end{equation*}

\textbf{Case 2.}
$i$ is not an exact power of 2.

Then $2^{\lfloor \lg i \rfloor} = 2^{\lfloor \lg (i - 1) \rfloor}$.

\begin{equation*}
    \Phi(D_i) - \Phi(D_{i - 1}) = 2
\end{equation*}

\begin{equation*}
\begin{split}
    \hat{c_i} & = c_i + \Phi(D_i) - \Phi(D_{i - 1}) \\
    & = 1 + 2 \\
    & = 3 \\
\end{split}
\end{equation*}

Hence the amortized cost per operation is $O(1)$.

\subsection*{17.3-3}

The idea is to let the potential be proportional to 
the sum of the height of every node in the min-heap.
Note that an binary heap is a complete binary tree.

\begin{equation*}
    \sum\limits_{j=1}^n \lfloor \lg j \rfloor
    \leq \lg (n!) \leq n \lg n
\end{equation*}

Let $\Phi$ be 
\begin{equation*}
    \Phi(D_i) = 
    \begin{cases}
        0 & \text{if $n_i = 0$,} \\
        k n_i \lg n_i & \text{if $n_i > 0$} \\
    \end{cases}
\end{equation*}

for some constant $k$
where $n_i$ is the number of nodes in $D_i$.
Also, we have
\begin{equation*}
    c_i \leq 
    \begin{cases}
        k_1 \lg n_i & \text{if $\proc{Insert}$ is performed in the $i$th operation 
            and $n_i \geq 2$,} \\
        k_2 \lg n_{i-1} & \text{if $\proc{Extract-Min}$ is performed in the $i$th operation 
            and $n_{i-1} \geq 2$} \\
    \end{cases}
\end{equation*}

Let $k = \func{max}(k_1,k_2)$.

\textbf{Case 1.}
$\proc{Insert}$ is performed in the $i$th operation.
Then $n_i - 1 = n_{i-1}$.

If $n_i = 1$,
\begin{equation*}
\begin{split}
    \hat{c_i} & = c_i + \Phi(D_i) - \Phi(D_{i - 1}) \\
    & = c_i \\
\end{split}
\end{equation*}

If $n_i \geq 2$,
\begin{equation*}
\begin{split}
    \hat{c_i} & = c_i + \Phi(D_i) - \Phi(D_{i - 1}) \\
    & \leq k \lg n_i + k n_i \lg n_i - k n_{i-1} \lg n_{i-1} \\
    & = k (\lg n_i + n_i \lg n_i - n_{i-1} \lg n_{i-1}) \\
    & = k (\lg n_i + n_i \lg n_i - (n_i - 1) \lg (n_i - 1)) \\
    & = k(\lg n_i + n_i \lg n_i - n_i \lg (n_i - 1) + \lg (n_i - 1)) \\
    & < k(2\lg n_i + n_i (\lg n_i - \lg (n_i - 1))) \\
\end{split}
\end{equation*}

Note that $\forall x \in \RR$, $1 + x \leq e^x$.
Then
\begin{equation*}
\begin{split}
    n_i (\lg n_i - \lg (n_i - 1)) & = n_i \lg \frac{n_i}{n_i - 1} \\
    & = n_i \lg (1 + \frac{1}{n_i - 1}) \\
    & \leq n_i \lg (e^{\frac{1}{n_i - 1}}) \\
    & = \frac{n_i}{n_i - 1} \lg e \\
    & = (1 + \frac{1}{n_i - 1}) \lg e \\
    & \leq 2 \lg e \\
\end{split}
\end{equation*}

Hence
\begin{equation*}
\begin{split}
    \hat{c_i} & < k(2\lg n_i + 2 \lg e)
\end{split}
\end{equation*}

We conclude $\hat{c_i} = O(\lg n)$ for $\proc{Insert}$.

\textbf{Case 2.}
$\proc{Extract-Min}$ is performed in the $i$th operation.
Then $n_{i-1} - 1 = n_i$.

If $n_{i-1} = 1$,
\begin{equation*}
\begin{split}
    \hat{c_i} & = c_i + \Phi(D_i) - \Phi(D_{i - 1}) \\
    & = c_i \\
\end{split}
\end{equation*}

If $n_{i-1} \geq 2$,
\begin{equation*}
\begin{split}
    \hat{c_i} & = c_i + \Phi(D_i) - \Phi(D_{i - 1}) \\
    & \leq k \lg n_{i-1} + k n_i \lg n_i - k n_{i-1} \lg n_{i-1} \\
    & = k (\lg n_{i-1} + n_i \lg n_i - n_{i-1} \lg n_{i-1}) \\
    & = k (\lg n_{i-1} + (n_{i-1} - 1) \lg (n_{i-1} - 1) - n_{i-1} \lg n_{i-1}) \\
    & < k (\lg n_{i-1} - \lg (n_{i-1} - 1)) \\
    & = k \lg (1 + \frac{1}{n_{i-1} - 1}) \\
    & \leq k \lg e^{\frac{1}{n_{i-1} - 1}} \\
    & = \frac{k}{n_{i-1} - 1} \lg e \\
\end{split}
\end{equation*}

We conclude $\hat{c_i} = O(1)$ for $\proc{Extract-Min}$.

\subsection*{17.3-4}

\begin{equation*}
    \Phi(D_{n}) - \Phi(D_{0}) = s_n - s_0
\end{equation*}

Since $\hat{c_i} = 2$,
\begin{equation*}
\begin{split}
    \sum\limits_{i = 1}^n c_i
    & = \sum\limits_{i = 1}^n \hat{c_i}
    - \Phi(D_{n}) + \Phi(D_{0}) \\
    & = 2n + s_0 - s_n
\end{split}
\end{equation*}

\subsection*{17.3-5}

\begin{equation*}
    \Phi(D_0) = b
\end{equation*}

Since $\hat{c_i} \leq 2$,
\begin{equation*}
\begin{split}
    \sum\limits_{i = 1}^n c_i
    & = \sum\limits_{i = 1}^n \hat{c_i}
    - \Phi(D_{n}) + \Phi(D_{0}) \\
    & \leq 2n + b - \Phi(D_{n})
\end{split}
\end{equation*}

Since $\Phi(D_{n}) \geq 0$,
\begin{equation*}
    \sum\limits_{i = 1}^n c_i \leq 2n + b
\end{equation*}

Since $n = \Omega(b)$,
\begin{equation*}
    \sum\limits_{i = 1}^n c_i = O(n)
\end{equation*}

\subsection*{17.3-6}

\begin{minted}[xleftmargin=20pt,linenos]{cpp}
template <typename T>
class Queue
{
public:
    void Enqueue(T& x);
    void Enqueue(T&& x);
    T Dequeue();
private:
    std::stack<T> s_a_;
    std::stack<T> s_b_;
};

template <typename T>
void Queue<T>::Enqueue(T& x)
{
    s_a_.push(x);
}

template <typename T>
void Queue<T>::Enqueue(T&& x)
{
    s_a_.emplace(std::move(x));
}

template <typename T>
T Queue<T>::Dequeue()
{
    if (s_b_.empty())
    {
        while (s_a_.empty() == false)
        {
            s_b_.emplace(std::move(s_a_.top()));
            s_a_.pop();
        }
    }
    T top = std::move(s_b_.top());
    s_b_.pop();
    return std::move(top);
}
\end{minted}

Assume each of $s\_a\_.push$ (or $emplace$), $s\_a\_.pop$, 
$s\_b\_.push$ (or $emplace$), $s\_b\_.pop$ costs $\$1$.
Then
\begin{equation*}
    c_i = 
    \begin{cases}
        1 & \text{if $\proc{Enqueue}$ is performed in the $i$th operation,} \\
        1 & \text{if $\proc{Dequeue}$ is performed in the $i$th operation
            and $D_{i-1}.s\_b\_$ is not empty,} \\
        2 \cdot (D_{i-1}.s\_a\_.size()) + 1 
            & \text{if $\proc{Dequeue}$ is performed in the $i$th operation
            and $D_{i-1}.s\_b\_$ is empty} \\
    \end{cases}
\end{equation*}

Let 
\begin{equation*}
    \Phi(D_i) = 3 \cdot (D_i.s\_a\_.size()) + (D_i.s\_b\_.size())
\end{equation*}

\textbf{Case 1.} 
$\proc{Enqueue}$ is performed in the $i$th operation.
\begin{equation*}
\begin{split}
    \hat{c_i} & = c_i + \Phi(D_i) - \Phi(D_{i-1}) \\
    & = 1 + 3 \cdot (D_i.s\_a\_.size() - D_{i-1}.s\_a\_.size()) 
        + (D_i.s\_b\_.size() - D_{i-1}.s\_b\_.size())\\
    & = 1 + 3 \cdot 1 + 0 \\
    & = 4 \\
\end{split}
\end{equation*}

\textbf{Case 2.} 
$\proc{Dequeue}$ is performed in the $i$th operation
and $D_{i-1}.s\_b\_$ is not empty.
\begin{equation*}
\begin{split}
    \hat{c_i} & = c_i + \Phi(D_i) - \Phi(D_{i-1}) \\
    & = 1 + 3 \cdot (D_i.s\_a\_.size() - D_{i-1}.s\_a\_.size()) 
        + (D_i.s\_b\_.size() - D_{i-1}.s\_b\_.size())\\
    & = 1 + 3 \cdot 0 - 1 \\
    & = 0 \\
\end{split}
\end{equation*}

\textbf{Case 3.} 
$\proc{Dequeue}$ is performed in the $i$th operation
and $D_{i-1}.s\_b\_$ is empty.
\begin{equation*}
\begin{split}
    \hat{c_i} & = c_i + \Phi(D_i) - \Phi(D_{i-1}) \\
    & = (2 \cdot (D_{i-1}.s\_a\_.size()) + 1)
         + 3 \cdot (D_i.s\_a\_.size() - D_{i-1}.s\_a\_.size()) 
        + (D_i.s\_b\_.size() - D_{i-1}.s\_b\_.size())\\
    & = (2 \cdot (D_{i-1}.s\_a\_.size()) + 1) 
        - 3 \cdot (D_{i-1}.s\_a\_.size()) + (D_{i-1}.s\_a\_.size() - 1) \\
    & = 0 \\
\end{split}
\end{equation*}

Thus, we conclude that the amortized cost of each
$\proc{Enqueue}$ and each $\proc{Dequeue}$ operation
is $O(1)$.

\subsection*{17.3-7}

Note that section 9.3 provides an approach of selection in worst-case linear time.

\begin{minted}[xleftmargin=20pt,linenos]{cpp}
class DataStructure
{
public:
    void Insert(int x);
    void DeleteLargerHalf();
    const std::vector<int>& Get() const;
private:
    std::vector<int> arr_;
};

void DataStructure::Insert(int x)
{
    arr_.push_back(x);
}

void DataStructure::DeleteLargerHalf()
{
    size_t median = arr_.size() >> 1;
    LinearSelect(arr_, 0, arr_.size() - 1, (arr_.size() - 1) >> 1);
    arr_.erase(arr_.begin() + median, arr_.end());
}

const std::vector<int>& DataStructure::Get() const
{
    return arr_;
}
\end{minted}

Assume
\begin{equation*}
    c_i = 
    \begin{cases}
        1 & \text{if $\proc{Insert}$ is performed in the $i$th operation,} \\
        n_{i-1} & \text{if $\proc{Delete-Larger-Half}$ 
            is performed in the $i$th operation} \\
    \end{cases}
\end{equation*}
where $n_i$ is $|S|$ after the $i$th operation.
Let
\begin{equation*}
    \Phi(D_i) = 2n_i
\end{equation*}
be the potential function of the data structure.

\textbf{Case 1.}
$\proc{Insert}$ is performed in the $i$th operation.
\begin{equation*}
\begin{split}
    \hat{c_i} & = c_i + \Phi(D_i) - \Phi(D_{i-1}) \\
    & = c_i+ 2(n_i - n_{i-1}) \\
    & = 1 + 2 \cdot 1 \\
    & = 3 \\
\end{split}
\end{equation*}

\textbf{Case 2.}
$\proc{Delete-Larger-Half}$ is performed in the $i$th operation.
\begin{equation*}
\begin{split}
    \hat{c_i} & = c_i + \Phi(D_i) - \Phi(D_{i-1}) \\
    & = c_i + 2(n_i - n_{i-1}) \\
    & = n_{i-1} + 2(\frac{n_{i-1}}{2} - n_{i-1}) \\
    & = n_{i-1} - n_{i-1} \\
    & = 0 \\
\end{split}
\end{equation*}

\section*{17.4}

\subsection*{17.4-1}

By Theorem 11.6 and Theorem 11.8, assuming uniform hashing,
for $alpha < 1$, we know the expected number of probes 
in an unsuccessful search is at most $\frac{1}{1 - \alpha}$
and in an successful search is at most 
$\frac{1}{\alpha} \ln \frac{1}{1 - \alpha}$.

\begin{equation*}
    \lim\limits_{\alpha \rightarrow 1^-} \frac{1}{1 - \alpha} = \infty
\end{equation*}

\begin{equation*}
    \lim\limits_{\alpha \rightarrow 1^-} 
    \frac{1}{\alpha} \ln \frac{1}{1 - \alpha} = \infty
\end{equation*}

Actually, when $\alpha = 1$, an unsuccessful search costs 
$\Theta(m) = \Theta(n)$.
If we can bound $\alpha$ above by some constant 
that is strictly less than $1$,
the expected time of an unsuccessful or successful search is bounded above
by some constant also.

Consider the function
\begin{equation*}
    \Phi_i = 2 \cdot num_i - \beta \cdot size_i
\end{equation*}
Let $\Phi_0 = 0$.

We just need to simply modify the conditional statement 
in line 4 of $\proc{Table-Insert}$ to 
\begin{codebox}
    \zi \If $T.num + 1 > \beta \cdot T.size$
\end{codebox}
where $\beta$ is some constant that is strictly less than $1$
and modify the base case of resizing table
by modifing line 3 of $\proc{Table-Insert}$ to 
\begin{codebox}
    \zi \If $T.size = \lceil \frac{1}{\beta} \rceil$
\end{codebox}

We want to show that one expansion (twice the size) is enough
in order to insert an element into a full table ($T.num + 1 > \beta \cdot T.size$).

\begin{lemma}
    Assume $num \geq 1$.
    $num \leq \beta \cdot size \Longrightarrow
    num + 1 \leq 2 \beta \cdot size$ 
\end{lemma}

\begin{proof}
    Note that $\forall x \geq 1, x + 1 \leq 2x$.
    \begin{equation*}
    \begin{split}
        num \leq \beta \cdot size & \Longleftrightarrow 2 \cdot num \leq 2 \beta \cdot size \\
        & \Longleftrightarrow num + 1 \leq 2 \cdot num \leq 2 \beta \cdot size \\
    \end{split}
    \end{equation*}
\end{proof}

\begin{claim}
    $num_i \leq \beta \cdot size_i$ for all $i$.
\end{claim}

\begin{proof}
    We prove by induction.
    WLOG, assume inserts are performed for all $i$.
    Then $num_{i+1} = i + 1$.
    
    \textit{(Base)}
    $k = 0$: $num_0 = 0 \leq \beta \cdot 0 = \beta \cdot size_i$

    $k = 1$: $num_1 = 1 \leq \beta \cdot \lceil \frac{1}{\beta} \rceil = \beta \cdot size_i$
    since $x \cdot \lceil \frac{1}{x} \rceil \geq x \cdot \frac{1}{x} = 1$ for all $x > 0$.

    \textit{(Induction)}
    Fix $k \geq 1$.
    Suppose that $num_k \leq \beta \cdot size_k$.

    \textbf{Case 1.}
    $num_k + 1 \leq \beta \cdot size_k$

    Then $size_{k+1} = size_k$.
    \begin{equation*}
        num_{k+1} = num_k + 1 \leq \beta \cdot size_k = \beta \cdot size_{k+1}
    \end{equation*}

    \textbf{Case 2.}
    $num_k + 1 > \beta \cdot size_k$
    
    Then $size_{k+1} = 2 \cdot size_k$.
    By lemma 1 and inductive hypothesis, we have
    $num_k + 1 \leq 2\beta \cdot size_k$.

    \begin{equation*}
        num_{k+1} = num_k + 1 \leq 2\beta \cdot size_k = \beta \cdot size_{k+1}
    \end{equation*}
\end{proof}

We also want to show that $\Phi(T)$ is always nonnegative.

\begin{claim}
    $\Phi_i \geq 0$ for all $i$
\end{claim}

\begin{proof}
    We prove by induction.
    WLOG, assume inserts are performed for all $i$.
    Then $num_{i+1} = i + 1$.

    \textit{(Base)}
    \begin{equation*}
        \Phi_0 = 0
    \end{equation*}
    \begin{equation*}
        \Phi_1 = 2 \cdot num_1 - \beta \cdot size_1
        = 2 \cdot 1 - \beta \cdot \lceil \frac{1}{\beta} \rceil
        > 2 - \beta \cdot (\frac{1}{\beta} + 1)
        = 1 - \beta > 0
    \end{equation*}

    \textit{(Induction)}
    Fix $k \geq 1$.
    Suppose that $\Phi_k = 2 \cdot num_k - \beta \cdot size_k \geq 0$.
    
    \textbf{Case 1.}
    $num_k + 1 \leq \beta \cdot size_k$

    Then $size_{k+1} = size_k$.
    \begin{equation*}
        \Phi_{k+1} = 2 \cdot num_{k+1} - \beta \cdot size_{k+1} 
        = 2 \cdot (num_k + 1) - \beta \cdot size_k 
        > 2 \cdot num_k - \beta \cdot size_k \overset{\text{IH}}{\geq} 0
    \end{equation*}
    
    \textbf{Case 2.}
    $num_k + 1 > \beta \cdot size_k$
    
    Then $size_{k+1} = 2 \cdot size_k$.
    \begin{equation*}
        \Phi_{k+1} = 2 \cdot num_{k+1} - \beta \cdot size_{k+1} 
        = 2 \cdot (num_k + 1) - 2 \beta \cdot size_k
        = 2 \cdot (num_k + 1 - \beta \cdot size_k)
        > 0
    \end{equation*}
    Since $num_k + 1 - \beta \cdot size_k > 0$
\end{proof}

We want to analysis the expected amortized cost.
By Theorem 11.6,
we assume
\begin{equation*}
    E[c_i] = 
    \begin{cases}
        1 & \text{if the $i$th insert operation does not trigger an expansion,} \\
        num_i & \text{if the $i$th insert operation does trigger an expansion} \\
    \end{cases}
\end{equation*}

If the $i$th insert operation does not trigger an expansion,
then we have $size_i = size_{i-1}$, and 
the expected amortized cost of the operation is
\begin{equation*}
\begin{split}
    E[\hat{c_i}] & = E[c_i] + \Phi_i - \Phi_{i-1} \\
    & = 1 + (2 \cdot num_i - \beta \cdot size_i) - (2 \cdot num_{i-1} - \beta \cdot size_{i-1}) \\
    & = 1 + (2 \cdot num_i - \beta \cdot size_i) - (2 \cdot (num_i - 1) - \beta \cdot size_i) \\
    & = 3 \text{ .} 
\end{split}
\end{equation*}

If the $i$th insert operation does trigger an expansion,
then we have $size_i = 2 \cdot size_{i-1}$, and 
the expected amortized cost of the operation is
\begin{equation*}
\begin{split}
    E[\hat{c_i}] & = E[c_i] + \Phi_i - \Phi_{i-1} \\
    & = num_i + (2 \cdot num_i - \beta \cdot size_i) - (2 \cdot num_{i-1} - \beta \cdot size_{i-1}) \\
    & = (num_{i-1} + 1) + (2 \cdot (num_{i-1} + 1) - \beta \cdot 2 \cdot size_{i-1}) 
        - (2 \cdot num_{i-1} - \beta \cdot size_{i-1}) \\
    & = 3 + num_{i-1} - \beta \cdot size_{i-1} \\
    & < 3 \text{. \quad\quad\quad\quad\quad (by claim 2)}
\end{split}
\end{equation*}
Note that $E[c_i] = num_i$, which is linear, in this case
because we need to copy all $num_{i-1}$ elements 
to the new allocated table.

\subsection*{17.4-2}

\begin{claim}
    $\forall num \geq 2, \frac{num}{size} \geq \frac{1}{2}
    \Longrightarrow \frac{num - 1}{size} \geq \frac{1}{4}$
\end{claim}

\begin{proof}
    \begin{equation*}
    \begin{split}
        \frac{num}{size} \geq \frac{1}{2}
        & \Longleftrightarrow
        2 \cdot num \geq size \\
        & \Longleftrightarrow
        4(num - 1) \geq 2 \cdot num \geq size 
        \text{ \quad\quad\quad\quad\quad 
        (since $num \geq 2$)}\\
        & \Longleftrightarrow
        \frac{num - 1}{size} \geq \frac{1}{4} \\
    \end{split}
    \end{equation*}
\end{proof}

Suppose that $\alpha_{i-1} \geq \frac{1}{2}$
and the $i$th operation is $\proc{Table-Delete}$.
Then $num_i = num_{i-1} - 1$.
By the claim, we know that a contraction will not be triggered
in the $i$th operation
if $\alpha_{i-1} \geq \frac{1}{2}$ and 
$num_{i-1} \geq 2$.
If $num_{i-1} = 1$, then $num_i = 0$, which is trivial.
Assume $num_{i-1} \geq 2$ in the following analysis.
Then $c_i = 1$ and $size_i = size_{i-1}$.

\textbf{Case 1.}
$\alpha_i < \frac{1}{2}$.

\begin{equation*}
\begin{split}
    \hat{c_i} & = c_i + \Phi_i - \Phi_{i-1} \\
    & = 1 + (\frac{size_i}{2} - num_i) - (2 \cdot num_{i-1} - size_{i-1}) \\
    & = 1 + (\frac{size_{i-1}}{2} - (num_{i-1} - 1)) - (2 \cdot num_{i-1} - size_{i-1}) \\
    & = 2 + \frac{3}{2} \cdot size_{i-1} - 3 \cdot num_{i-1} \\
    & = 2 + \frac{3}{2} \cdot size_{i-1} - 3 \alpha_{i-1} \cdot size_{i-1} \\
    & \leq 2 + \frac{3}{2} \cdot size_{i-1} - \frac{3}{2} \cdot size_{i-1} \\
    & = 2 \\
\end{split}
\end{equation*}

\textbf{Case 2.}
$\alpha_i \geq \frac{1}{2}$.

\begin{equation*}
\begin{split}
    \hat{c_i} & = c_i + \Phi_i - \Phi_{i-1} \\
    & = 1 + (2 \cdot num_i - size_i) - (2 \cdot num_{i-1} - size_{i-1}) \\
    & = 1 + (2 \cdot (num_{i-1} - 1) - size_{i-1}) - (2 \cdot num_{i-1} - size_{i-1}) \\
    & = -1 \\
\end{split}
\end{equation*}

\subsection*{17.4-3}

Suppose that $\proc{Table-Delete}$ is performed in the $i$th operation.
Then $num_i = num_{i-1} - 1$.

\textbf{Case 1.}
$\frac{num_{i-1}-1}{size_{i-1}} \geq \frac{1}{3}$
(a contraction is not triggered in the $i$th operation).

Then we have $c_i = 1$ and $size_i = size_{i-1}$.

\begin{equation*}
\begin{split}
    \hat{c_i} & = c_i + \Phi_i - \Phi_{i-1} \\
    & = c_i + | 2 \cdot num_i - size_i | - | 2 \cdot num_{i-1} - size_{i-1} | \\
    & = 1 + | 2 \cdot (num_{i-1} - 1) - size_{i-1} | - | 2 \cdot num_{i-1} - size_{i-1} | \\
    & \overset{\triangle}{\leq} 1 + (| 2 \cdot num_{i-1} - size_{i-1} | + | - 2 |) 
        - | 2 \cdot num_{i-1} - size_{i-1} | \\
    & = 3 \\
\end{split}
\end{equation*}

\textbf{Case 2.}
$\frac{num_{i-1}-1}{size_{i-1}} < \frac{1}{3}$
(a contraction is triggered in the $i$th operation).

Then we have $c_i = num_i + 1 = num_{i-1}$ and $size_i = \frac{2}{3} \cdot size_{i-1}$.

\begin{equation*}
\begin{split}
    \hat{c_i} & = c_i + \Phi_i - \Phi_{i-1} \\
    & = c_i + | 2 \cdot num_i - size_i | - | 2 \cdot num_{i-1} - size_{i-1} | \\
    & = num_{i-1} + | 2 \cdot (num_{i-1} - 1) - \frac{2}{3} \cdot size_{i-1} | 
        - | 2 \cdot num_{i-1} - size_{i-1} | \\
    & = num_{i-1} + | 2 \cdot num_{i-1} - \frac{2}{3} \cdot size_{i-1} - 2 | 
        - | 2 \cdot num_{i-1} - size_{i-1} | \\
\end{split}
\end{equation*}

\begin{lemma}
    \begin{equation*}
        2 \cdot num_{i-1} - \frac{2}{3} \cdot size_{i-1} - 2 < 0
    \end{equation*}
\end{lemma}

\begin{proof}
    \begin{equation*}
    \begin{split}
        \frac{num_{i-1}-1}{size_{i-1}} < \frac{1}{3}
        & \Longrightarrow 3 \cdot (num_{i-1} - 1) < size_{i-1} \\
        & \Longrightarrow 2 \cdot (num_{i-1} - 1) < \frac{2}{3} \cdot size_{i-1} \\
        & \Longrightarrow 2 \cdot num_{i-1} - \frac{2}{3} \cdot size_{i-1} - 2 < 0 \\
    \end{split}
    \end{equation*}
\end{proof}

\begin{lemma}
    \begin{equation*}
        \forall num_{i-1} \geq 3, 
        2 \cdot num_{i-1} - size_{i-1} < 0
    \end{equation*}
\end{lemma}

\begin{proof}
    \begin{equation*}
    \begin{split}
        \frac{num_{i-1}-1}{size_{i-1}} < \frac{1}{3}
        & \Longrightarrow 3 \cdot (num_{i-1} - 1) < size_{i-1} \\
        & \Longrightarrow 2 \cdot num_{i-1} - 3 + num_{i-1} < size_{i-1} \\
        & \Longrightarrow 2 \cdot num_{i-1} - size_{i-1} + (num_{i-1} - 3) < 0 \\
        & \Longrightarrow 2 \cdot num_{i-1} - size_{i-1} < 3 - num_{i-1} \\
    \end{split}
    \end{equation*}
    Clearly, $\forall num_{i-1} \geq 3, 3 - num_{i-1} \leq 0$.
\end{proof}

The subcase of $num_{i-1} < 3$ is trivial.
Assume $num_{i-1} \geq 3$ in the later analysis.

Then
\begin{equation*}
\begin{split}
    \hat{c_i} & = num_{i-1} + | 2 \cdot num_{i-1} - \frac{2}{3} \cdot size_{i-1} - 2 | 
        - | 2 \cdot num_{i-1} - size_{i-1} | \\
    & = num_{i-1} - ( 2 \cdot num_{i-1} - \frac{2}{3} \cdot size_{i-1} - 2 ) 
        \overset{(--)}{+} ( 2 \cdot num_{i-1} - size_{i-1} ) \\
    & = (num_{i-1} - \frac{1}{3} \cdot size_{i-1} - 1) + 3 \\
\end{split}
\end{equation*}

\begin{lemma}
    \begin{equation*}
        num_{i-1} - \frac{1}{3} \cdot size_{i-1} - 1 < 0
    \end{equation*}
\end{lemma}

\begin{proof}
    \begin{equation*}
    \begin{split}
        \frac{num_{i-1}-1}{size_{i-1}} < \frac{1}{3}
        & \Longrightarrow num_{i-1} - 1 < \frac{1}{3} \cdot size_{i-1} \\
        & \Longrightarrow num_{i-1} - \frac{1}{3} \cdot size_{i-1} - 1 < 0 \\
    \end{split}
    \end{equation*}
\end{proof}

Hence
\begin{equation*}
    \hat{c_i} = (num_{i-1} - \frac{1}{3} \cdot size_{i-1} - 1) + 3 < 3
\end{equation*}

\section*{Chapter 17 Problems}

\subsection*{17-1}

\subsubsection*{(a)}

\begin{minted}[xleftmargin=20pt,linenos]{cpp}
template <typename T>
void BitReversal(std::vector<T>& arr)
{
    size_t k, tmp, i, n, rev;
    n = arr.size();
    std::vector<bool> counter(n, false);
    tmp = n;
    k = -1;
    while (tmp)
    {
        tmp = tmp >> 1;
        ++k;
    }
    for (i = 0; i < n; ++i)
    {
        if (counter[i] == false)
        {
            rev = Rev(k, i);
            std::swap(arr[i], arr[rev]);
            counter[i] = true;
            counter[rev] = true;
        }
    }
}
\end{minted}

\subsubsection*{(b)}

Note that
\begin{equation*}
    \func{rev}_k(\langle a_{k-1}, a_{k-2}, \cdots, a_0 \rangle)
     = \langle a_0, a_1, \cdots, a_{k-1} \rangle
\end{equation*}

In order to find $rev_k(a) + 1$, 
we just need to call $\proc{Increment}$ on $rev_k(a)$.
We observed that we can modify $\proc{Increment}$
by starting iteration from the high order bit to the low order bit
in order to find $rev_k( rev_k(a) + 1 ) + 1$.

\begin{minted}[xleftmargin=20pt,linenos]{cpp}
size_t BitReversedIncrement(size_t k, size_t a)
{
    size_t i;
    i = 1 << (k - 1);
    while (i > 0 && (a & i) != 0)
    {
        a = a & ( ~ i );
        i = i >> 1;
    }
    a = a | i;
    return a;
}
\end{minted}

Similar to the analysis of $\proc{Increment}$,
successive call to $\proc{Bit-Reversed-Increment}$
produce the sequence in a total of $O(n)$ time.

\subsubsection*{(c)}

Yes.
We can modify our $\proc{Bit-Reversed-Increment}$
by precompute value of $1 << (k - 1)$ before the first call
in order to prevent recomputing this value in each call. 

Observed the operation of $i = i >> 1$ always following 
flipping the bit back to $0$.
Hence we can use the same analysis in this situation.

\subsection*{17-2}

\begin{minted}[xleftmargin=20pt,linenos]{cpp}
template <typename T>
class DynamicBinarySearch
{
public:
    using Iterators = std::pair
        <typename std::list< std::vector<T> >::iterator, 
        typename std::vector<T>::iterator>;
    Iterators Search(const T& key);
    void Insert(const T& key);
    void Delete(const Iterators& its);
private:
    std::list< std::vector<T> > arrays_;
};
\end{minted}
    
\subsubsection*{(a)}

\begin{minted}[xleftmargin=20pt,linenos]{cpp}
template <typename T>
typename DynamicBinarySearch<T>::Iterators 
    DynamicBinarySearch<T>::Search(const T& key)
{
    size_t n, i;
    n = arrays_.size();
    for (typename std::list< std::vector<T> >::iterator it = arrays_.begin(); 
        it != arrays_.end(); ++it)
    {
        typename std::vector<T>::iterator sub_it = 
            std::lower_bound(it->begin(), it->end(), key);
        if (sub_it != it->end() && *sub_it == key)
            return std::make_pair(it, sub_it);
    }
    throw std::out_of_range("the container does not have an element "
                            "with the specified key");
}
\end{minted}

In the worst case, $n_i = 1$ for all $i = 0, 1, \cdots, k - 1$
and the binary search is unsuccessful.
Then the running time in the worst case is 
\begin{equation*}
    \Theta(\sum\limits_{i = 0}^{k - 1} n_i \lg(2^i))
    = \Theta(\sum\limits_{i = 0}^{k - 1} i)
    = \Theta(k^2)
    = \Theta(\lg^2 n)
\end{equation*}

\subsubsection*{(b)}

\begin{minted}[xleftmargin=20pt,linenos]{cpp}
template <typename T>
void DynamicBinarySearch<T>::Insert(const T& key)
{
    size_t n;
    std::vector<T> merged_arr, tmp;
    typename std::list< std::vector<T> >::iterator it = arrays_.begin();
    n = 1;
    merged_arr.push_back(key);
    while (it != arrays_.end() && it->size() > 0)
    {
        n >>= 1;
        tmp = std::move(merged_arr);
        merged_arr.reserve(n);
        std::merge(tmp.begin(), tmp.end(), 
            it->begin(), it->end(), 
            std::back_inserter(merged_arr));
        it->clear();
        ++it;
    }
    if (it != arrays_.end())
        *it = std::move(merged_arr);
    else
        arrays_.emplace_back(std::move(merged_arr));
}
\end{minted}

Suppose that the $i$th operation clears (or merges) $t_i$ arrays. 
We have $0 \leq t_i \leq k - 1$ for all $i$.
Note that merge two arrays with size $m$ and $n$ runs in $\Theta(m + n)$.
The sum of the sizes of arrays $A_0, A_1, \cdots, A_{j-1}$ is
\begin{equation*}
    total\_size(0,j-1) = \sum\limits_{h = 0}^{j - 1} 2^h = 2^j - 1
\end{equation*}
In $i$th operation, we merge arrays $A_0, A_1, \cdots, A_{t_i-1}$
and insert the new element,
and the running time is
\begin{equation*}
\begin{split}
    \Theta(1 + \sum_{j = 0}^{t_i - 1} (2^j + total\_size(0, j - 1)))
    & = \Theta(1 + \sum_{j = 0}^{t_i - 1} (2^j + 2^j - 1)) \\
    & = \Theta(1 - t_i + 2 \cdot \sum_{j = 0}^{t_i - 1} 2^j) \\
    & = \Theta(1 - t_i + 2 \cdot (2^{t_i} - 1)) \\ 
    & = \Theta(2^{t_i + 1} - t_i - 1) \\
    & = \Theta(2^{t_i}) \\
\end{split}
\end{equation*}
Clearly, the running time of the worst case is 
\begin{equation*}
    \Theta(2^{k - 1}) = \Theta(2^{\lceil \lg (n + 1) \rceil - 1})
    = \Theta(n)
\end{equation*}
when $t_i = k - 1$.

We analysis amortized cost by using aggregate analysis.
Similar to the analysis of $\proc{Increment}$,
array $A[j]$ clears (merge with another $2^j$ elements 
and insert them into $A[j+1]$) 
$\lfloor n / 2^j \rfloor$ times
in a sequence of $n$ $\proc{Insert}$ operations on an initially
empty container (array of arrays),
and the running time of each time in $\Theta(2 \cdot 2^j) = \Theta(2^j)$.
Hence the total cost of a sequence of $n$ $\proc{Insert}$ operations is
\begin{equation*}
    \Theta(\sum\limits_{j = 0}^{k - 1} \lfloor \frac{n}{2^j} \rfloor 2^j)
    = \Theta(n k) = \Theta(n \lg n)
\end{equation*}
Then the amortized cost of each $\proc{Insert}$ is $\Theta(\lg n)$.

\subsubsection*{(c)}

\begin{minted}[xleftmargin=20pt,linenos]{cpp}
template <typename T>
void DynamicBinarySearch<T>::Delete(const Iterators& its)
{
    size_t n;
    // find the full array with the smallest size 
    // (i.e. the full array with smallest index in arrays_)
    typename std::list< std::vector<T> >::iterator 
        first_full_it = arrays_.begin();
    while (first_full_it->size() == 0)
    {
        ++first_full_it;
    }
    // delete the element and refill the array with first_full_it->back()
    typename std::vector<T>::iterator processing_element_it = its.second,
        target_element_it = std::lower_bound
        (its.first->begin(), its.first->end(), first_full_it->back());
    if (target_element_it > processing_element_it)
    {
        ++processing_element_it;
        while (target_element_it != processing_element_it)
        {
            *(processing_element_it - 1) = std::move(*processing_element_it);
            ++processing_element_it;
        }
        *(processing_element_it - 1) = std::move(first_full_it->back());
    }
    else
    {
        while (target_element_it != processing_element_it)
        {
            *processing_element_it = std::move(*(processing_element_it - 1));
            --processing_element_it;
        }
        *processing_element_it = std::move(first_full_it->back());
    }
    // split first_full_it and assign them into arrays with smaller index in arrays_
    n = 1;
    typename std::vector<T>::iterator element_it = first_full_it->begin();
    for (typename std::list< std::vector<T> >::iterator it = arrays_.begin();
        it != first_full_it; ++it)
    {
        it->assign(std::move_iterator(element_it), 
            std::move_iterator(element_it) + n);
        element_it = element_it + n;
        n <<= 1;
    }
    first_full_it->clear();
    // remove empty arrays (optional)
    if (arrays_.back().empty())
    {
        arrays_.pop_back();
    }
}
\end{minted}

In the worst case, delete operation takes $\Theta(n)$ time.

\subsection*{17-3}

\subsubsection*{(a)}

\begin{minted}[xleftmargin=20pt,linenos]{cpp}
void RebuildTreeTranverse(Node* root, std::vector<Node*>& elements)
{
    if (root != nullptr)
    {
        RebuildTreeTranverse(root->left, elements);
        elements.push_back(root);
        RebuildTreeTranverse(root->right, elements);
    }
}

Node* RebuildTreeBuild(std::vector<Node*>& elements, int lower, int upper)
{
    int middle;
    if (lower > upper)
        return nullptr;
    middle = lower + ((upper - lower) >> 1);
    elements[middle]->size = upper - lower + 1;
    elements[middle]->left = RebuildTreeBuild(elements, lower, middle - 1);
    elements[middle]->right = RebuildTreeBuild(elements, middle + 1, upper);
    return elements[middle];
}

Node* RebuildTree(Node* root)
{
    std::vector<Node*> elements;
    elements.reserve(root->size);
    RebuildTreeTranverse(root, elements);
    return RebuildTreeBuild(elements, 0, root->size - 1);
}
\end{minted}

\subsubsection*{(b)}

Let $T(n)$ be the running time of a search on 
a $\alpha$-balanced binary search tree with $n$ elements.
We have
\begin{equation*}
    T(n) \leq T(\alpha \cdot n) + \Theta(1)
    = T(\frac{n}{1/\alpha}) + \Theta(1)
\end{equation*}
Since $\alpha < 1$, we have $1/\alpha > 1$.
By the master theorem,
we have
\begin{equation*}
    T(\frac{n}{1/\alpha}) + \Theta(1) = \Theta(\lg n)
\end{equation*}
Hence we have
\begin{equation*}
    T(n) = O(\lg n)
\end{equation*}

\subsubsection*{(c)}

Since $c$ is a sufficiently large constant, 
we may assume $c > 0$.
$\Delta(x)$ is an absolute value.
Obviously, $\Phi(T)$ is always nonnegative. 

In order to show that a $1/2$-balanced tree $T$ has potential $0$,
we need to show that 
\begin{equation*}
    \Delta(x) = | x.left.size - x.right.size | \leq 1
\end{equation*}
for all $x \in T$.

Let $x \in T$.
WLOG, assume $x.left.size \geq x.right.size$,
so we want to show that 
\begin{equation*}
    x.left.size - x.right.size \leq 1
\end{equation*}
Note that 
\begin{equation*}
    x.size = x.left.size + x.right.size + 1
\end{equation*}
Since $T$ is a $1/2$-balanced tree,
we have 
\begin{equation*}
    x.left.size \leq \frac{1}{2} \cdot x.size
\end{equation*}
Then
\begin{equation*}
\begin{split}
    x.left.size - x.right.size 
    & = x.left.size - (x.size - x.left.size - 1) \\
    & = 2 \cdot x.left.size - x.size + 1 \\
    & \leq 2 \cdot \frac{1}{2} \cdot x.size - x.size + 1 \\
    & = 1 \\
\end{split}
\end{equation*}

\subsubsection*{(d)}

Note that we rebuild the subtree to be $1/2$-balanced,
instead to $\alpha$-balanced.

Denote $\Phi_j$ as the potential of $T$ after the $j$th operation.
Denote $size_j(w)$ as the $w.size$ after the $j$th operation 
for all $w \in T$.
Denote $\Delta_j(w)$ as the $\Delta(w)$ after the $j$th operation 
for all $w \in T$.
Suppose that $T$ is not $\alpha$-balanced after the $(i-1)$th operation
and is $1/2$-balanced after the $i$th operation.
By part (c), we have $\Phi_i = 0$.
Let $x$ be the root node of $T$ after the $(i-1)$th operation.
We have
\begin{equation*}
    \Phi_{i-1} \geq c \cdot \Delta_{i-1}(x)
\end{equation*}
WLOG, assume $size_{i-1}(x.left) \geq size_{i-1}(x.right)$.
Since $T$ is not $\alpha$-balanced after the $(i-1)$th operation,
\begin{equation*}
    size_{i-1}(x.left) > \alpha \cdot size_{i-1}(x)
\end{equation*}
must be true.
Then
\begin{equation*}
\begin{split}
    \Delta_{i-1}(x) & = size_{i-1}(x.left) - size_{i-1}(x.right) \\
    & = size_{i-1}(x.left) - (size_{i-1}(x) - size_{i-1}(x.left) - 1) \\
    & = 2 \cdot size_{i-1}(x.left) - size_{i-1}(x) + 1 \\
    & > 2 \alpha \cdot size_{i-1}(x) - size_{i-1}(x) + 1 \\
    & = (2 \alpha - 1) \cdot size_{i-1}(x) + 1 \\
    & > (2 \alpha - 1) \cdot size_{i-1}(x) \\
\end{split}
\end{equation*}
Suppose that $size_{i-1}(x) = m$.
That is we are rebuilding an $m$-node subtree in the $i$th operation.
Hence the amortized cost of the $i$th operation is 
\begin{equation*}
\begin{split}
    m + \Phi_i - \Phi_{i-1} & = m - \Phi_{i-1} \\
    & \leq m - c \cdot \Delta_{i-1}(x) \\
    & < m - c \cdot (2 \alpha - 1) \cdot m \\
\end{split}
\end{equation*}
In order to let the amortized time be $O(1)$,
we let $c \geq \frac{1}{2 \alpha - 1}$.

\subsubsection*{(e)}

Suppose that we perform insert or delete in the $i$th operation.
Let $h$ be the height of $T$,
By part (b), we derive the $h = O(\lg n)$.
Then we pay $O(\lg n)$ for actual inserting or deleting the node.
If we do not rebuild the tree, 
then $\Phi_i - \Phi_{i-1} \leq h + 1 = O(\lg n)$.
If we rebuild the tree,
then we rebuild the subtreee rooted at the highest 
non-$\alpha$-balanced node, and, by part (d), 
the amortized cost of rebuild is $O(1)$.

\subsection*{17-4}

\subsubsection*{(a)}

We notice that the only locations to have repeated coloring 
are fixup functions 
($\proc{RB-Insert-Fixup}$ and $\proc{RB-Delete-Fixup}$)
and, by the analysis of insert and delete,
we know the $\While$ loop of fixup functions repeat
only if case 1 of $\proc{RB-Insert-Fixup}$ or
case 2 of $\proc{RB-Delete-Fixup}$ occurs.

\textbf{insert}

In order to causes $\Omega(\lg n)$ color changes,
we want the case 1 of $\proc{RB-Insert-Fixup}$ 
occurs $\Omega(\lg n)$ times.
Consider the situation of the case 1 occurs every iteration.
This situation occurs only if
the color of parent of $z$ is RED
and the color of uncle of $z$ is RED
unless the parent of $z$ is the root node in every iteration;
i.e. ($z == T.root$ or ($z.p.color == RED$ and $y.color == RED$));
equivalent to ($z == T.root$ or 
($z.p.p.left.color == RED$ and $z.p.p.right.color == RED$))
in every iteration.
For instance, consider a full, complete red-black tree where 
all nodes on level $0$ are black,
all nodes on level $1$ are red, 
all nodes on level $2$ are black, 
all nodes on level $3$ are red, 
all nodes on level $4$ are black, 
all nodes on level $5$ are red\dots

\textbf{delete}

In order to causes $\Omega(\lg n)$ color changes,
we want the case 2 of $\proc{RB-Delete-Fixup}$ 
occurs $\Omega(\lg n)$ times.
Consider the situation of we are deleting a leaf node,
and the case 2 occurs every iteration.
For instance, consider a full, complete red-black tree where 
all nodes in the tree are black.

\subsubsection*{(b)}

By the analysis of insert and delete.
the answer is case 2 and 3 for $\proc{RB-Insert}$
and case 1, 3, and 4 for $\proc{RB-Delete}$.

\subsubsection*{(c)}

Suppose that the current iteration in $\proc{RB-Insert-Fixup}$ are applying case 1.
Right before the case 1 is applied
(right before line 5),
we know $z.p.color == RED$ and $y.color == RED$,
so $z.p.p.color == BLACK$.
After the case 1 is applied 
(excluede the changing of identity of $z$;
i.e. right asfter line 7),
we blacken two red nodes and redden one black nodes.
Hence the number of red nodes in $T$ decreases by 1.
That is $\Phi(T') = \Phi(T) - 1$.

\subsubsection*{(d)}

\begin{tabular}{c|c|c}
    type & location & potential change \\ 
    \hline
    node insertion & $\proc{RB-Insert}$: line 9 - 16 & $+1$ \\
    color change & $\proc{RB-Insert-Fixup}$: line 5 - 7 (case 1) & $-1$ \\
    rotation & $\proc{RB-Insert-Fixup}$: line 11 (case 2) & $0$ \\
    color change & $\proc{RB-Insert-Fixup}$: line 12 - 13 (case 3) & $0$ \\
    rotation & $\proc{RB-Insert-Fixup}$: line 14 (case 3) & $0$ \\
\end{tabular}

Note that we assume that $1$ unit of potential can pay for
the structural modifications performed 
by any of the three cases of $\proc{RB-Insert-Fixup}$.

We conclude that insertion costs $1$ and causes potential increases by $1$,
applying case 1 costs $1$ and causes potential decreases by $1$,
applying case 2 costs $1$ and potential unchanged,
and applying case 3 costs $1$ and potential unchanged.

\subsubsection*{(e)}

Each time case 1 (nonterminating) is applied
we use the potential change of $-1$ to 
pay the cost of structural modifications in applying case 1.
Case 2 and 3 (terminating) can be performed at most once 
in each call since they are terminating cases,
so the structural modifications of applying cases 2 and 3 are at most $2$.
Hence the amortized number of structural modifications for each call is
\begin{equation*}
\begin{split}
    & (\text{insertion cost}) + (\text{insertion potential change}) + 
    m \cdot ((\text{case 1 cost}) + (\text{case 1 potential change})) + \\
    & (\text{case 2 cost}) + (\text{case 2 potential change}) + 
    (\text{case 3 cost}) + (\text{case 3 potential change}) \\
    \leq & (1) + (1) + m \cdot ((1) + (-1)) + (1) + (0) + (1) + (0)
    = 4
\end{split}
\end{equation*}

\subsubsection*{(f)}

Suppose that the current iteration in $\proc{RB-Insert-Fixup}$ are applying case 1.
In this case, the color of $z.p$ will change from RED to BLACK at line 5,
the color of $y$ will change from RED to BLACK at line 6,
and the color of $z.p.p$ will change from BLACK to RED at line 7.
We know $z.p$ has one red child $z$ since $z.p.color$ is RED before applying case 1
and $z$ is the processing node (so we allow $z$ to be red temporary),
so $w(z.p)$ unchanged ($z.p$ is red $\rightarrow$ $z.p$ is black and has one red child).
We know $y$ has no red children since $y.color$ is RED before applying case 1,
so $w(y)$ increases by $1$ ($y$ is red $\rightarrow$ $y$ is black and has no red child).
We know $z.p.p$ has two red children ($z.p$ and $y$) before applying case 1
and become red after applying case 1,
so $w(z.p.p)$ decreases by $2$ 
($z.p.p$ is black and has two red children $\rightarrow$ $z.p.p$ is red).
We assume the color of $z.p.p.p$ is red here;
otherwise, the loop will terminate after this iteration.
Hence $\Phi(T') = \Phi(T) - 1$.

We use this potential change of $-1$ to 
pay the cost of structural modifications in applying case 1 each time,
so the amortized number of structural modifications in nonterminating cases is $0$.
Since we only perform constant times structural modifications in the terminating cases,
and potential changes are bounded by constant also,
we conclude that the amortized number of structural modifications is $O(1)$.

\subsubsection*{(g)}

Suppose that the current iteration in $\proc{RB-Delete-Fixup}$ are applying case 2.
Since case 2 are applied, we know that $x.color == BLACK$ by line 1,
$w.left.color == BLACK$ and $w.left.color == BLACK$ by line 9,
and $w.color == BLACK$ by line 4-5.
In this case, the color of $w$ will change from BLACK to RED at line 10.
Thus, we have 
$w(w)$ decreases by $1$ ($w$ is black and has no red children $\rightarrow$ $w$ is red).
If $x.p$ is black, then we have $w(x.p)$ decreases by $1$ 
($x.p$ is black and has no red children $\rightarrow$ $x.p$ is black and has one red child).
If $x.p$ is red (actually, the loop will terminate after this iteration if $x.p$ is red), 
then we have $w(x.p)$ unchanged ($x.p$ is red $\rightarrow$ $x.p$ is red).
Hence $\Phi(T') \leq \Phi(T) - 1$.

By the similar approach of part (f),
we conclude that the amortized number of structural modifications is $O(1)$.

\subsubsection*{(h)}

By part (f) and (g), we know the \textbf{amorted} number of structural modifications performed by
any call of $\proc{RB-Insert-Fixup}$ or $\proc{RB-Delete-Fixup}$ is $O(1)$.
Also, the \textbf{amorted} number of structural modifications performed by 
lines 1 - 16 of $\proc{RB-Insert}$ and lines 1 - 21 of $\proc{RB-Delete}$ is $O(1)$.
Therefore, in the worst case, 
any sequence of $m$ $\proc{RB-Insert}$ and $\proc{RB-Delete}$ operations
on an initially empty red-black tree causes $O(m)$ structural modifications in total.

\centerline{\textbf{Updating...}}

\end{document}